{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impoart Necessary Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "# Function:\n",
    "# Combine all inputs/columns into one string\n",
    "\n",
    "def combine_col(input_data):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from collections import Counter\n",
    "    import nltk\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    '''Combine all inputs/columns into one string.\n",
    "       Returns a DataFrame'''\n",
    "    input_data['combined_data'] = input_data['name'] + \" \" + input_data['description'] + \" \" + input_data['brand_category']\n",
    "    return input_data\n",
    "# Function:\n",
    "# Tokenize string and remove stopwords\n",
    "\n",
    "def remove_stopwords(input_data):\n",
    "    '''Tokenize string.\n",
    "        Returns a list.'''\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from collections import Counter\n",
    "    import nltk\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # combine all columns - call combin_col() function\n",
    "    data2 = combine_col(input_data)\n",
    "\n",
    "    # remove_stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    import re\n",
    "\n",
    "    regex_word_tokenize = nltk.RegexpTokenizer(r\"(\\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)\")\n",
    "    nltk_stopwords = list((set(stopwords.words('english'))))\n",
    "\n",
    "    # remove \"unknown\" since in the full_data dataset, some brand_category are unknown\n",
    "    nltk_stopwords.append('unknown')\n",
    "\n",
    "    result2 = []\n",
    "    for line in data2['combined_data']:\n",
    "        filtered_words = []\n",
    "        if isinstance(line, str):\n",
    "            line = re.sub(r'\\d+\\+*[\\- ]*[\\-]*',' ',line)\n",
    "            for word in regex_word_tokenize.tokenize(line):\n",
    "                if word.isdigit() == False:\n",
    "                    if word.lower() not in nltk_stopwords:\n",
    "                        filtered_words.append(word.lower())\n",
    "            result2.append(\" \".join(filtered_words))\n",
    "        else:\n",
    "            result2.append(np.nan)\n",
    "    data2['rm_sw'] = result2\n",
    "    return data2\n",
    "\n",
    "def lemmatize_word(input_data):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from collections import Counter\n",
    "    import nltk\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    d1 = remove_stopwords(input_data)\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk_stopwords = list((set(stopwords.words('english'))))\n",
    "    regex_word_tokenize = nltk.RegexpTokenizer(r\"(\\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    result3 = []\n",
    "    for i in range(len(d1['rm_sw'])):\n",
    "        lemmatized = []\n",
    "        if isinstance(d1['rm_sw'].iloc[i],str):\n",
    "            for word in regex_word_tokenize.tokenize(d1['rm_sw'].iloc[i]):\n",
    "                lemmatized.append(lemmatizer.lemmatize(word))\n",
    "            result3.append(\" \".join(lemmatized))\n",
    "        else:\n",
    "            result3.append(d1['rm_sw'].iloc[i])\n",
    "    d1['lemmatized'] = result3\n",
    "    d1['final'] = d1['lemmatized'] + \" \" + d1['brand']\n",
    "    d1['final_list'] = d1['final'].str.split()\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## DATA-PREPROCESSING\n",
    "# Import data\n",
    "full_data = pd.read_csv('Full+data.csv')\n",
    "# Remove some columns in full+data\n",
    "x = full_data.columns[14:256]\n",
    "full_data = full_data.drop(x,axis=1)\n",
    "# Clean/organize values in these columns\n",
    "full_data['product_id'] = full_data['product_id'].str.upper()\n",
    "full_data['brand'] = full_data['brand'].str.title()\n",
    "full_data['name'] = full_data['name'].str.title()\n",
    "full_data['description'] = full_data['description'].str.title()\n",
    "full_data['brand_category'] = full_data['brand_category'].str.title()\n",
    "full_data['details'] = full_data['details'].str.title()\n",
    "full_data['brand_canonical_url'] = full_data['brand_canonical_url'].str.lower()\n",
    "# Import extra_data.csv and standardize string in the columns\n",
    "extra = pd.read_csv('extra_data.csv')\n",
    "extra['product_id'] = extra['product_id'].str.upper()\n",
    "extra['brand'] = extra['brand'].str.title()\n",
    "extra['name'] = extra['name'].str.title()\n",
    "extra['description'] = extra['description'].str.title()\n",
    "extra['brand_category'] = extra['brand_category'].str.title()\n",
    "extra['details'] = extra['details'].str.title()\n",
    "extra['notes'] = extra['notes'].str.lower()\n",
    "extra['brand_canonical_url'] = extra['brand_canonical_url'].str.lower()\n",
    "# Import first part of tagged data from USC+Product+Attribute+Data+03302020.xlsx\n",
    "attr1 = pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx', sheet_name = 'Data')\n",
    "attr1['attribute_name'] = attr1['attribute_name'].str.lower()\n",
    "attr1['attribute_value'] = attr1['attribute_value'].str.lower()\n",
    "attr2 = pd.read_csv('usc_additional_tags.csv')\n",
    "attr2['attribute_name'] = attr2['attribute_name'].str.lower()\n",
    "attr2['attribute_value'] = attr2['attribute_value'].str.lower()\n",
    "attr1['attribute_value'] = attr1['attribute_value'].str.replace(r'[^a-zA-Z0-9<]', \"\")\n",
    "attr2['attribute_value'] = attr2['attribute_value'].str.replace(r'[^a-zA-Z0-9<]', \"\")\n",
    "# Standardizing attribute_name in 2nd tagged doc to the same as the 1st one\n",
    "key = ['additionalcolor', 'beltbucklematerial', 'beltbuckleshape', 'beltclosure',\n",
    "       'beltmaterial', 'beltwidth', 'calfwidth', 'category', 'classbelts',\n",
    "       'classblazerscoatsandjackets', 'classbooties', 'classboots', 'classdress',\n",
    "       'classflats', 'classhandbags', 'classjumpsuitandromper',\n",
    "       'classmulesandslides', 'classpantsandleggings', 'classpumpsandheels',\n",
    "       'classsandals', 'classshorts', 'classskirts', 'classslippers',\n",
    "       'classsneakersandathletic', 'classsunglasses', 'classwedges',\n",
    "       'closureblazerscoatsandjackets', 'closurehandbag', 'closureonepiece',\n",
    "       'closurepantsandleggings', 'closureshoe', 'closureshorts', 'closureskirts',\n",
    "       'closuresweater', 'closuretop', 'color', 'drycleanonly', 'embellishment',\n",
    "       'fit', 'gender', 'heelheight', 'heelshape', 'legstyle', 'legstylejeans',\n",
    "       'lengthblazers', 'lengthblazerscoatsandjackets', 'lengthcoatsandjackets',\n",
    "       'lengthjeans', 'lengthonepiece', 'lengthpantsandleggings', 'lengthshorts',\n",
    "       'lengthskirts', 'lengthtop', 'material', 'materialclothing', 'materialpurse',\n",
    "       'neckline', 'occasion', 'pattern', 'primarycolor', 'print', 'rise', 'risejeans',\n",
    "       'shaftheight', 'sheer', 'shoewidth', 'sizing', 'sleevelength', 'strap',\n",
    "       'strapmaterial', 'style', 'subcategoryaccessory',\n",
    "       'subcategoryblazerscoatsandjackets', 'subcategorybottom',\n",
    "       'subcategoryonepiece', 'subcategoryshoe', 'subcategorysweater',\n",
    "       'subcategorysweatshirtandhoodie', 'subcategorytop', 'sunglassframematerial',\n",
    "       'sweatshirtandhoodieclosure', 'toeexposure', 'toestyle', 'trend',\n",
    "       'uppermaterial', 'wash']\n",
    "value = ['additional color', 'beltbuckle_material', 'belt_buckle_shape', 'belt_closure',\n",
    "         'belt_material', 'belt_width', 'calf_width', 'category', 'class_belts',\n",
    "         'class_blazers_coats_and_jackets', 'class_booties', 'class_boots', 'class_dress',\n",
    "         'class_flats', 'class_handbags', 'class_jumpsuit_and_romper',\n",
    "         'class_mules_and_slides', 'class_pants_and_leggings', 'class_pumps_and_heels',\n",
    "         'class_sandals', 'class_shorts', 'class_skirts', 'class_slippers',\n",
    "         'class_sneakers_and_athletic', 'class_sunglasses', 'class_wedges',\n",
    "         'closure_blazers_coats_and_jackets', 'closure_handbag', 'closure_onepiece',\n",
    "         'closure_pants_and_leggings', 'closure_shoe', 'closure_shorts', 'closure_skirts',\n",
    "         'closure_sweater', 'closure_top', 'color', 'dry_clean_only', 'embellishment',\n",
    "         'fit', 'gender', 'heel_height', 'heel_shape', 'leg_style', 'leg_style_jeans',\n",
    "         'length_blazers', 'length_blazers_coats_and_jackets', 'length_coats_and_jackets',\n",
    "         'length_jeans', 'length_onepiece', 'length_pants_and_leggings', 'length_shorts',\n",
    "         'length_skirts', 'length_top', 'material', 'material_clothing', 'material_purse',\n",
    "         'neckline', 'occasion', 'pattern', 'primary_color', 'print', 'rise', 'rise_jeans',\n",
    "         'shaft_height', 'sheer', 'shoe_width', 'sizing', 'sleeve_length', 'strap',\n",
    "         'strapm_aterial_', 'style', 'subcategory_accessory',\n",
    "         'subcategory_blazers_coats_and_jackets', 'subcategory_bottom',\n",
    "         'subcategory_onepiece', 'subcategory_shoe', 'subcategory_sweater',\n",
    "         'subcategory_sweatshirt_and_hoodie', 'subcategory_top', 'sunglass_frame_material',\n",
    "         'sweatshirt_and_hoodie_closure', 'toe_exposure', 'toe_style', 'trend',\n",
    "         'upper_material', 'wash']\n",
    "update_dict = dict(zip(key,value))\n",
    "attr2['attribute_name'] = attr2['attribute_name'].map(update_dict)\n",
    "attr = pd.concat([attr1, attr2]).sort_values(by = 'product_id')\n",
    "attr = attr.drop_duplicates(keep = 'last').sort_values(by = 'product_id')\n",
    "unique_prod_id = list(set(full_data['product_id'])) + list(set(extra['product_id']))\n",
    "# Select related columns \n",
    "data1 = full_data[['product_id','brand','name','description','brand_category','details']]\n",
    "data2 = extra[['product_id','brand','name','description','brand_category','details']]\n",
    "data = pd.concat([data1, data2])\n",
    "data['brand_category'] = data['brand_category'].str.replace(r'(/|,|:)',' ')\n",
    "data['brand_category'] = data['brand_category'].str.lower()\n",
    "data['brand'] = data['brand'].str.title()\n",
    "data['name'] = data['name'].str.title()\n",
    "data['description'] = data['description'].str.title()\n",
    "data['description'] = data['description'].str.replace('\\n',' ')\n",
    "data['details'] = data['details'].str.title()\n",
    "data['details'] = data['details'].str.replace('\\n',' ')\n",
    "data.drop(labels = ['details'], inplace = True, axis = 1)\n",
    "data = data.replace(np.nan, '', regex=True)\n",
    "data = data.replace(r'(unknown|Unknown)', '', regex=True)\n",
    "data.reset_index(inplace = True)\n",
    "data = data.drop(labels = ['index'], axis = 1)\n",
    "final = lemmatize_word(data)\n",
    "final = pd.merge(left = final, right = attr, left_on = 'product_id', right_on = 'product_id', how = 'inner')\n",
    "output_data = final[['product_id','final','final_list','product_color_id','attribute_name','attribute_value']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "unique_prod_doc = output_data['final'].drop_duplicates(keep = 'first')\n",
    "corpus = list(unique_prod_doc.values)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "training_tfidf_vectors = vectorizer.transform(list(output_data['final']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict rest of full_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, auc, classification_report\n",
    "\n",
    "to_predict = lemmatize_word(data)\n",
    "# to_predict = to_predict[to_predict['product_id'].isin(attr['product_id'].unique())==False]\n",
    "to_predict['vectorized_doc'] = list(vectorizer.transform(to_predict['final']).toarray())\n",
    "# to_predict_csv = to_predict[['product_id','brand','name','description','brand_category','final']]\n",
    "to_predict_df = to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry Clean\n",
    "dry_clean_only = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='dry_clean_only'].drop_duplicates()\n",
    "dry_clean_onlys = output_data['attribute_value'][output_data['attribute_name']=='dry_clean_only'].unique()\n",
    "dry_clean_only['dry_clean_only_encode'] = 0\n",
    "dry_clean_only['dry_clean_only_encode'][dry_clean_only['attribute_value']=='yes'] = 1\n",
    "dry_clean_only['vectorized_doc'] = list(vectorizer.transform(dry_clean_only['final']).toarray())\n",
    "\n",
    "data1 = dry_clean_only\n",
    "X_train = np.array(data1['vectorized_doc'])\n",
    "y_train = np.array(data1['dry_clean_only_encode'])\n",
    "\n",
    "dry_clean_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "dry_clean_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "predict_data = to_predict_df\n",
    "dry_clean_only_prediction = dry_clean_model.predict(list(predict_data['vectorized_doc']))\n",
    "to_predict_df['dry_clean_only'] = dry_clean_only_prediction\n",
    "to_predict_df['dry_clean_only'] = to_predict_df['dry_clean_only'].map({0:'no',1:'yes'})\n",
    "# to_predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category\n",
    "category = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='category'].drop_duplicates()\n",
    "categories = output_data['attribute_value'][output_data['attribute_name']=='category'].unique()\n",
    "category['category_encode'] = ''\n",
    "categories_dict = dict(zip(categories,range(0,8)))\n",
    "category['category_encode'] = category['attribute_value'].map(categories_dict)\n",
    "category['vectorized_doc'] = list(vectorizer.transform(category['final']).toarray())\n",
    "\n",
    "data1 = category\n",
    "X_train = np.array(data1['vectorized_doc'])\n",
    "y_train = np.array(data1['category_encode'])\n",
    "\n",
    "category_model = DecisionTreeClassifier(max_depth = 2)\n",
    "category_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "predict_data = to_predict_df\n",
    "category_prediction = category_model.predict(list(predict_data['vectorized_doc']))\n",
    "to_predict_df['category'] = category_prediction\n",
    "to_predict_df['category'] = to_predict_df['category'].map(dict(zip(range(0,8),categories)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embellishment\n",
    "embellishment_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='embellishment']\n",
    "embellishments = output_data['attribute_value'][output_data['attribute_name']=='embellishment'].unique()\n",
    "embellishment = embellishment_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "for e in embellishments:\n",
    "    embellishment[e] = 0\n",
    "    id_with_e = embellishment_output_data['product_id'][embellishment_output_data['attribute_value']==e].unique()\n",
    "    embellishment[e][embellishment['product_id'].isin(id_with_e)] = 1\n",
    "embellishment['vectorized_doc'] = list(vectorizer.transform(embellishment['final']).toarray())\n",
    "\n",
    "data1 = embellishment\n",
    "for i in embellishments:\n",
    "    X_train = np.array(data1['vectorized_doc'])\n",
    "    y_train = np.array(data1[i])\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    model.fit(list(X_train), list(y_train))\n",
    "    \n",
    "    predict_data = to_predict_df    \n",
    "    embellishment_prediction = model.predict(list(predict_data['vectorized_doc']))\n",
    "    to_predict_df[f'embellishment-{i}'] = embellishment_prediction\n",
    "    to_predict_df[f'embellishment-{i}'] = to_predict_df[f'embellishment-{i}'].map({0:'no',1:'yes'}, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style\n",
    "style_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='style']\n",
    "styles = output_data['attribute_value'][output_data['attribute_name']=='style'].unique()\n",
    "style = style_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "for e in styles:\n",
    "    style[e] = 0\n",
    "    id_with_e = style_output_data['product_id'][style_output_data['attribute_value']==e].unique()\n",
    "    style[e][style['product_id'].isin(id_with_e)] = 1\n",
    "style['vectorized_doc'] = list(vectorizer.transform(style['final']).toarray())\n",
    "\n",
    "data1 = style\n",
    "for i in styles:\n",
    "    X_train = np.array(data1['vectorized_doc'])\n",
    "    y_train = np.array(data1[i].values)\n",
    "    \n",
    "    style_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    style_model.fit(list(X_train), list(y_train))\n",
    "    \n",
    "    predict_data = to_predict_df\n",
    "    style_prediction = style_model.predict(list(predict_data['vectorized_doc']))\n",
    "    to_predict_df[f'style-{i}'] = style_prediction\n",
    "    to_predict_df[f'style-{i}'] = to_predict_df[f'style-{i}'].map({0:'no',1:'yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occasion\n",
    "occasion_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='occasion']\n",
    "occasions = output_data['attribute_value'][output_data['attribute_name']=='occasion'].unique()\n",
    "occasion = occasion_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "for e in occasions:\n",
    "    occasion[e] = 0\n",
    "    id_with_e = occasion_output_data['product_id'][occasion_output_data['attribute_value']==e].unique()\n",
    "    occasion[e][occasion['product_id'].isin(id_with_e)] = 1\n",
    "occasion['vectorized_doc'] = list(vectorizer.transform(occasion['final']).toarray())\n",
    "\n",
    "data1 = occasion\n",
    "for i in occasions:\n",
    "    X_train = np.array(data1['vectorized_doc'])\n",
    "    y_train = np.array(data1[i].values)\n",
    "    \n",
    "    ocassion_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    ocassion_model.fit(list(X_train), list(y_train))\n",
    "    \n",
    "    predict_data = to_predict_df\n",
    "    ocassion_prediction = ocassion_model.predict(list(predict_data['vectorized_doc']))\n",
    "    to_predict_df[f'occasion-{i}'] = ocassion_prediction\n",
    "    to_predict_df[f'occasion-{i}'] = to_predict_df[f'occasion-{i}'].map({0:'no',1:'yes'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export full_data tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_to_predict_df = to_predict_df.drop(labels = ['combined_data','rm_sw','lemmatized',\n",
    "                                                   'final','final_list','vectorized_doc'],\n",
    "                                         axis = 1)\n",
    "# final_to_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "final_to_predict_df.to_csv('full_and_extra_data_tagged.csv',doublequote=True, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a function that will take in user's input and return an prediction output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to prepare user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_input(name, description, brand_category):\n",
    "    input_df = pd.Series(data = [name, description, brand])\n",
    "    input_df = input_df.to_frame().T\n",
    "    input_df.columns = ['name','description','brand']\n",
    "    input_df['combined_data'] = input_df['name'] + \" \" + input_df['description'] + \" \" + input_df['brand']\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    regex_word_tokenize = nltk.RegexpTokenizer(r\"(\\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)\")\n",
    "    nltk_stopwords = list((set(stopwords.words('english'))))\n",
    "    nltk_stopwords.append('unknown')\n",
    "    input_df['combined_data'] = re.sub(r'\\d+\\+*[\\- ]*[\\-]*',' ',input_df['combined_data'][0])\n",
    "    filtered_words = []\n",
    "    if isinstance(input_df['combined_data'][0], str):\n",
    "        for word in regex_word_tokenize.tokenize(input_df['combined_data'][0]):\n",
    "            if word.isdigit() == False:\n",
    "                if word.lower() not in nltk_stopwords:\n",
    "                    filtered_words.append(word.lower())\n",
    "        result = \" \".join(filtered_words)\n",
    "    input_df['rm_sw'] = result\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk_stopwords = list((set(stopwords.words('english'))))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    input_df['final'] = lemmatizer.lemmatize((input_df['rm_sw'][0]))\n",
    "    input_df['vectorized_doc'] = list(vectorizer.transform(list(input_df['final'])).toarray())\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to predict user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_predict(name, description, brand):\n",
    "    # Build model\n",
    "    from sklearn import preprocessing, linear_model\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, auc, classification_report\n",
    "\n",
    "    to_predict_df = prepare_input(name, description, brand)\n",
    "    # Dry Clean\n",
    "    dry_clean_only = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='dry_clean_only'].drop_duplicates()\n",
    "    dry_clean_onlys = output_data['attribute_value'][output_data['attribute_name']=='dry_clean_only'].unique()\n",
    "    dry_clean_only['dry_clean_only_encode'] = 0\n",
    "    dry_clean_only['dry_clean_only_encode'][dry_clean_only['attribute_value']=='yes'] = 1\n",
    "    dry_clean_only['vectorized_doc'] = list(vectorizer.transform(dry_clean_only['final']).toarray())\n",
    "\n",
    "    data1 = dry_clean_only\n",
    "    X_train = np.array(data1['vectorized_doc'])\n",
    "    y_train = np.array(data1['dry_clean_only_encode'])\n",
    "\n",
    "    dry_clean_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    dry_clean_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "    predict_data = to_predict_df\n",
    "    dry_clean_only_prediction = dry_clean_model.predict(list(predict_data['vectorized_doc']))\n",
    "    to_predict_df['dry_clean_only'] = dry_clean_only_prediction\n",
    "    to_predict_df['dry_clean_only'] = to_predict_df['dry_clean_only'].map({0:'no',1:'yes'})\n",
    "\n",
    "    # Category\n",
    "    category = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='category'].drop_duplicates()\n",
    "    categories = output_data['attribute_value'][output_data['attribute_name']=='category'].unique()\n",
    "    category['category_encode'] = ''\n",
    "    categories_dict = dict(zip(categories,range(0,8)))\n",
    "    category['category_encode'] = category['attribute_value'].map(categories_dict)\n",
    "    category['vectorized_doc'] = list(vectorizer.transform(category['final']).toarray())\n",
    "\n",
    "    data1 = category\n",
    "    X_train = np.array(data1['vectorized_doc'])\n",
    "    y_train = np.array(data1['category_encode'])\n",
    "\n",
    "    category_model = DecisionTreeClassifier(max_depth = 2)\n",
    "    category_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "    predict_data = to_predict_df\n",
    "    category_prediction = category_model.predict(list(predict_data['vectorized_doc']))\n",
    "    to_predict_df['category'] = category_prediction\n",
    "    to_predict_df['category'] = to_predict_df['category'].map(dict(zip(range(0,8),categories)))\n",
    "\n",
    "    # Embellishment\n",
    "    embellishment_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='embellishment']\n",
    "    embellishments = output_data['attribute_value'][output_data['attribute_name']=='embellishment'].unique()\n",
    "    embellishment = embellishment_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "    for e in embellishments:\n",
    "        embellishment[e] = 0\n",
    "        id_with_e = embellishment_output_data['product_id'][embellishment_output_data['attribute_value']==e].unique()\n",
    "        embellishment[e][embellishment['product_id'].isin(id_with_e)] = 1\n",
    "    embellishment['vectorized_doc'] = list(vectorizer.transform(embellishment['final']).toarray())\n",
    "\n",
    "    data1 = embellishment\n",
    "    for i in embellishments:\n",
    "        X_train = np.array(data1['vectorized_doc'])\n",
    "        y_train = np.array(data1[i])\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "        model.fit(list(X_train), list(y_train))\n",
    "\n",
    "        predict_data = to_predict_df    \n",
    "        embellishment_prediction = model.predict(list(predict_data['vectorized_doc']))\n",
    "        to_predict_df[f'embellishment-{i}'] = embellishment_prediction\n",
    "        to_predict_df[f'embellishment-{i}'] = to_predict_df[f'embellishment-{i}'].map({0:'no',1:'yes'}, )\n",
    "\n",
    "    # Style\n",
    "    style_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='style']\n",
    "    styles = output_data['attribute_value'][output_data['attribute_name']=='style'].unique()\n",
    "    style = style_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "    for e in styles:\n",
    "        style[e] = 0\n",
    "        id_with_e = style_output_data['product_id'][style_output_data['attribute_value']==e].unique()\n",
    "        style[e][style['product_id'].isin(id_with_e)] = 1\n",
    "    style['vectorized_doc'] = list(vectorizer.transform(style['final']).toarray())\n",
    "\n",
    "    data1 = style\n",
    "    for i in styles:\n",
    "        X_train = np.array(data1['vectorized_doc'])\n",
    "        y_train = np.array(data1[i].values)\n",
    "\n",
    "        style_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "        style_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "        predict_data = to_predict_df\n",
    "        style_prediction = style_model.predict(list(predict_data['vectorized_doc']))\n",
    "        to_predict_df[f'style-{i}'] = style_prediction\n",
    "        to_predict_df[f'style-{i}'] = to_predict_df[f'style-{i}'].map({0:'no',1:'yes'})\n",
    "\n",
    "    # Occasion\n",
    "    occasion_output_data = output_data[['product_id','final','attribute_name','attribute_value']][output_data['attribute_name']=='occasion']\n",
    "    occasions = output_data['attribute_value'][output_data['attribute_name']=='occasion'].unique()\n",
    "    occasion = occasion_output_data[['product_id','final','attribute_name']].drop_duplicates()\n",
    "    for e in occasions:\n",
    "        occasion[e] = 0\n",
    "        id_with_e = occasion_output_data['product_id'][occasion_output_data['attribute_value']==e].unique()\n",
    "        occasion[e][occasion['product_id'].isin(id_with_e)] = 1\n",
    "    occasion['vectorized_doc'] = list(vectorizer.transform(occasion['final']).toarray())\n",
    "\n",
    "    data1 = occasion\n",
    "    for i in occasions:\n",
    "        X_train = np.array(data1['vectorized_doc'])\n",
    "        y_train = np.array(data1[i].values)\n",
    "\n",
    "        ocassion_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "        ocassion_model.fit(list(X_train), list(y_train))\n",
    "\n",
    "        predict_data = to_predict_df\n",
    "        ocassion_prediction = ocassion_model.predict(list(predict_data['vectorized_doc']))\n",
    "        to_predict_df[f'occasion-{i}'] = ocassion_prediction\n",
    "        to_predict_df[f'occasion-{i}'] = to_predict_df[f'occasion-{i}'].map({0:'no',1:'yes'})\n",
    "    return to_predict_df.drop(labels = ['combined_data','rm_sw','final','vectorized_doc'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please input your item information here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER ITEM'S NAME, DESCRIPTION, BRAND_CATEGORY AS STRINGS\n",
    "name = input(\"Item name (in string): \")\n",
    "description =  input(\"Item description (in string): \")\n",
    "brand_category =  input(\"Item brand_category (in string): \")\n",
    "\n",
    "to_predict(name, description, brand_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEMO #####\n",
    "# ENTER ITEM'S NAME, DESCRIPTION, BRAND_CATEGORY AS STRINGS\n",
    "name = input(\"Item name (in string): \")\n",
    "description =  input(\"Item description (in string): \")\n",
    "brand_category =  input(\"Item brand_category (in string): \")\n",
    "\n",
    "to_predict(name, description, brand_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
